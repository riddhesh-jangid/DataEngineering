{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67230f60-62ea-4f16-b6c4-a8961d7fb695",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "imports"
    }
   },
   "outputs": [],
   "source": [
    "import posixpath\n",
    "import json\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType\n",
    "import uuid\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d716f10f-8e89-40e3-b768-4ca4cbe6efa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"table_name\", \"\")\n",
    "dbutils.widgets.text(\"run_date_values\", \"\")\n",
    "\n",
    "table_name = dbutils.widgets.get(\"table_name\")\n",
    "run_date_values = dbutils.widgets.get(\"run_date_values\").split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f4e7941-28e5-4eb6-abfa-ad29c1b6dba4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "widgets"
    }
   },
   "outputs": [],
   "source": [
    "pipeline_name = 'bronze_ingestion'\n",
    "pipeline_run_id = str(uuid.uuid4())\n",
    "base_path = f'/Volumes/otc/volumn/landingfiles/{table_name}/'\n",
    "\n",
    "run_date = ['run_date='+d for d in run_date_values]\n",
    "run_mode = 'single' if len(run_date)==1 else 'multiple'\n",
    "\n",
    "start_ts = datetime.utcnow()\n",
    "run_status = 'failed'\n",
    "\n",
    "\n",
    "ingestion_params = f\"\"\"\n",
    "{pipeline_name},\n",
    "{pipeline_run_id},\n",
    "{table_name},\n",
    "{run_date_values},\n",
    "{run_mode},\n",
    "{base_path}\n",
    "\"\"\"\n",
    "\n",
    "print(ingestion_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "386bed23-496f-4e6a-80f6-9e9a2353c57c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "read_options_json & read_landing_csv"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Read source_schema_registry for table\n",
    "    source_schema_registry = spark.table('otc_config.config.source_schema_registry')\\\n",
    "        .filter( F.col('table_name')==table_name )\n",
    "        \n",
    "    # Get read_options_json    \n",
    "    read_options_json = source_schema_registry.select('read_options_json').collect()[0][0]  \n",
    "    read_options_json = json.loads(read_options_json)\n",
    "\n",
    "    # Get table_schema \n",
    "    table_schema = source_schema_registry.select('schema_ddl').collect()[0][0]\n",
    "\n",
    "\n",
    "    def get_ddl_cols(schema):\n",
    "        ddl_cols = [[]]\n",
    "        ddl_cols[0] = [schema.split(\" \")[0], '']\n",
    "        for i, x in enumerate(schema.split(\" \")[1:-1]):\n",
    "            ddl_cols[i][1] = x.rsplit(',',1)[0]\n",
    "            ddl_cols.append( ['', ''] )\n",
    "            ddl_cols[i+1][0] = x.rsplit(',',1)[1]\n",
    "\n",
    "        ddl_cols[-1][1] = schema.split(\" \")[-1]\n",
    "        return ddl_cols\n",
    "\n",
    "\n",
    "    # Get csv as dataframe\n",
    "    def read_landing_files(PATH, read_options_json, schema):\n",
    "        # Get name and type separately\n",
    "        ddl_cols = get_ddl_cols(schema)\n",
    "\n",
    "        # Read csv from landing\n",
    "        if table_name=='src_payments' or table_name=='src_shipments':\n",
    "            return_df = spark.read.format('json')\\\n",
    "                .options(**read_options_json)\\\n",
    "                            .load( PATH )\n",
    "        else:\n",
    "            return_df = spark.read.format('csv')\\\n",
    "                .options(**read_options_json)\\\n",
    "                            .load( PATH )\n",
    "\n",
    "        # Enforce schema\n",
    "        return_df = return_df.select(  *[ F.col(col_name).cast(col_type).alias(col_name) for col_name, col_type in ddl_cols] )\n",
    "        \n",
    "        return_df = return_df.withColumn('source_file', F.input_file_name())\n",
    "\n",
    "        return return_df\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    dbutils.notebook.exit({\n",
    "        'notebook_run_status': 'failed',\n",
    "        'ingestion_params': ingestion_params,\n",
    "        'remark': f'Reading config failed: {e}'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c5a64ab-f068-4f0d-be28-d51ee1f37e4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_in_bronze(write_df, table_name, run_date):\n",
    "    try:\n",
    "        # Delete the partition\n",
    "        delete_query = f\" DELETE FROM otc.bronze.{table_name} WHERE run_date='{run_date}' \"\n",
    "        spark.sql(delete_query)\n",
    "\n",
    "        # Write to bronze\n",
    "        write_df.write.mode('append').saveAsTable(f'otc.bronze.{table_name}')\n",
    "\n",
    "        return {\n",
    "            'status': 'success',\n",
    "            'error': ''\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Failed due to ',e)\n",
    "        return {\n",
    "            'status': 'failed',\n",
    "            'error': e\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bf1b8d7-1b72-46de-8ca2-476184280203",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get src_customer"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    if run_mode=='single':\n",
    "        FULL_PATH = posixpath.join( base_path, run_date[0] )\n",
    "        src_bronze = read_landing_files(FULL_PATH, read_options_json, table_schema)\n",
    "\n",
    "        # Adding run_date column\n",
    "        run_date_values = [date.split(\"=\")[-1] for date in run_date]\n",
    "        src_bronze = src_bronze.withColumn('run_date', F.lit( run_date_values[0] ).cast('date') )\n",
    "\n",
    "    elif run_mode=='multiple':\n",
    "        FULL_PATH = base_path\n",
    "        src_bronze = read_landing_files(FULL_PATH, read_options_json, table_schema)\n",
    "\n",
    "        # Filtering on run_date columns\n",
    "        run_date_values = [date.split(\"=\")[-1] for date in run_date]\n",
    "        src_bronze = src_bronze.filter(  F.col('run_date').isin( run_date_values ) )\n",
    "        \n",
    "\n",
    "except Exception as e:\n",
    "    dbutils.notebook.exit({\n",
    "        'notebook_run_status': 'failed',\n",
    "        'ingestion_params': ingestion_params,\n",
    "        'remark': f'Reading landing files failed: {e}'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a4df29b-90a5-4820-9696-bb40410fe652",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Main Ingestion Code"
    }
   },
   "outputs": [],
   "source": [
    "for rn in run_date_values:\n",
    "    wr_src_bronze = src_bronze.filter( F.col('run_date')==rn )\n",
    "\n",
    "    rows_read = wr_src_bronze.count()\n",
    "    end_ts = datetime.utcnow()\n",
    "\n",
    "    ingestion_status = write_in_bronze(wr_src_bronze, table_name, rn)\n",
    "\n",
    "    insert_into_pipeline_run = f\"\"\"\n",
    "        INSERT INTO otc.audit.pipeline_run (pipeline_name, pipeline_run_id, table_name, run_date, run_mode, start_ts, end_ts, status, rows_read , error_message) \n",
    "        VALUES (\n",
    "            '{pipeline_name}',\n",
    "            '{pipeline_run_id}',\n",
    "            '{table_name}',\n",
    "            '{rn}',\n",
    "            '{run_mode}',\n",
    "            '{start_ts}',\n",
    "            '{end_ts}',\n",
    "            '{ingestion_status['status']}',\n",
    "            '{rows_read}',\n",
    "            '{ingestion_status['error']}'\n",
    "            )\n",
    "    \"\"\"\n",
    "\n",
    "    spark.sql(insert_into_pipeline_run)\n",
    "\n",
    "\n",
    "# Return success message to ADF pipeline\n",
    "dbutils.notebook.exit({\n",
    "    'notebook_run_status': 'success',\n",
    "    'ingestion_params': ingestion_params,\n",
    "    'remark': 'Ingestion Completed'\n",
    "    })"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8164781401386120,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_bronze_ingestion",
   "widgets": {
    "run_date_values": {
     "currentValue": "2026-01-04",
     "nuid": "d1f8b137-9b82-4f06-a2a6-f894ee7d1f76",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "run_date_values",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "run_date_values",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "table_name": {
     "currentValue": "src_payments",
     "nuid": "123bf7cd-f1e4-4cc5-acd8-2328b23b510d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "table_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "table_name",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
